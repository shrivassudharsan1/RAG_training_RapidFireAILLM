{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e4c769cc",
      "metadata": {
        "id": "e4c769cc"
      },
      "source": [
        "### Install and Initialize RapidFire AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d0e23e77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0e23e77",
        "outputId": "2351678f-7b13-4748-d9cd-a7c0953f7cc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rapidfireai\n",
            "  Downloading rapidfireai-0.12.8-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: flask>=3.1.1 in /usr/local/lib/python3.12/dist-packages (from rapidfireai) (3.1.2)\n",
            "Collecting flask-cors>=6.0.1 (from rapidfireai)\n",
            "  Downloading flask_cors-6.0.2-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting waitress>=3.0.2 (from rapidfireai)\n",
            "  Downloading waitress-3.0.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting jq>=1.10.0 (from rapidfireai)\n",
            "  Downloading jq-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from rapidfireai) (0.3.8)\n",
            "Collecting jedi>=0.16 (from rapidfireai)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting uv>=0.8.14 (from rapidfireai)\n",
            "  Downloading uv-0.9.21-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask>=3.1.1->rapidfireai) (1.9.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask>=3.1.1->rapidfireai) (8.3.1)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask>=3.1.1->rapidfireai) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask>=3.1.1->rapidfireai) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask>=3.1.1->rapidfireai) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from flask>=3.1.1->rapidfireai) (3.1.4)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->rapidfireai) (0.8.5)\n",
            "Downloading rapidfireai-0.12.8-py3-none-any.whl (46.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m46.4/46.4 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cors-6.0.2-py3-none-any.whl (13 kB)\n",
            "Downloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jq-1.10.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (757 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m757.1/757.1 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uv-0.9.21-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m38.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading waitress-3.0.2-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.2/56.2 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: waitress, uv, jq, jedi, flask-cors, rapidfireai\n",
            "Successfully installed flask-cors-6.0.2 jedi-0.19.2 jq-1.10.0 rapidfireai-0.12.8 uv-0.9.21 waitress-3.0.2\n",
            "Created directory: /content/rapidfireai/logs\n",
            "üîß Initializing RapidFire AI project...\n",
            "------------------------------\n",
            "Initializing project...\n",
            "Colab environment detected, installing evals packages\n",
            "Installing packages from /usr/local/lib/python3.12/dist-packages/setup/evals/requirements-colab.txt...\n",
            "‚úÖ Successfully installed packages from /usr/local/lib/python3.12/dist-packages/setup/evals/requirements-colab.txt\n",
            "Getting tutorial notebooks...\n",
            "Copying tutorial notebooks from /usr/local/lib/python3.12/dist-packages/tutorial_notebooks to ./tutorial_notebooks...\n",
            "‚úÖ Successfully copied notebooks to ./tutorial_notebooks\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    import rapidfireai\n",
        "    print(\"‚úÖ rapidfireai already installed\")\n",
        "except ImportError:\n",
        "    !pip install rapidfireai  # Takes 1 min\n",
        "    !rapidfireai init --evals # Takes 1 min"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Intall and intialize mlflow (to plot metrics)"
      ],
      "metadata": {
        "id": "rSnMUj4iDc8Z"
      },
      "id": "rSnMUj4iDc8Z"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "Fm4CuRX5MzhZ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fm4CuRX5MzhZ",
        "outputId": "f22eb9ea-c76d-45b4-a508-075ec882700b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mlflow\n",
            "  Downloading mlflow-3.8.1-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-skinny==3.8.1 (from mlflow)\n",
            "  Downloading mlflow_skinny-3.8.1-py3-none-any.whl.metadata (31 kB)\n",
            "Collecting mlflow-tracing==3.8.1 (from mlflow)\n",
            "  Downloading mlflow_tracing-3.8.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: Flask-CORS<7 in /usr/local/lib/python3.12/dist-packages (from mlflow) (6.0.2)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.17.2)\n",
            "Requirement already satisfied: cryptography<47,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
            "Collecting docker<8,>=4.0.0 (from mlflow)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting graphene<4 (from mlflow)\n",
            "  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting gunicorn<24 (from mlflow)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting huey<3,>=2.5.0 (from mlflow)\n",
            "  Downloading huey-2.5.5-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<23,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.3)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.45)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (6.2.4)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (8.3.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (3.1.2)\n",
            "Collecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==3.8.1->mlflow)\n",
            "  Downloading databricks_sdk-0.76.0-py3-none-any.whl.metadata (40 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (0.123.10)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (2.12.3)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (1.2.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (6.0.3)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (2.32.5)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (0.5.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.8.1->mlflow) (0.38.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<47,>=43.0.0->mlflow) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.0.3)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.4)\n",
            "Collecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_core-3.2.7-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n",
            "  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (3.2.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<47,>=43.0.0->mlflow) (2.23)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (2.43.0)\n",
            "Requirement already satisfied: starlette<0.51.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow) (0.50.0)\n",
            "Requirement already satisfied: annotated-doc>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.8.1->mlflow) (0.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (4.0.12)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.8.1->mlflow) (3.23.0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.8.1->mlflow) (0.58b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=2.0.0->mlflow-skinny==3.8.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (3.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.8.1->mlflow) (2025.11.12)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.12/dist-packages (from uvicorn<1->mlflow-skinny==3.8.1->mlflow) (0.16.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.8.1->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (4.9.1)\n",
            "Requirement already satisfied: anyio<5,>=3.6.2 in /usr/local/lib/python3.12/dist-packages (from starlette<0.51.0,>=0.40.0->fastapi<1->mlflow-skinny==3.8.1->mlflow) (4.12.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.8.1->mlflow) (0.6.1)\n",
            "Downloading mlflow-3.8.1-py3-none-any.whl (9.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.1/9.1 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_skinny-3.8.1-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlflow_tracing-3.8.1-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huey-2.5.5-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m76.9/76.9 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading databricks_sdk-0.76.0-py3-none-any.whl (774 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m774.7/774.7 kB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_core-3.2.7-py3-none-any.whl (207 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\n",
            "Installing collected packages: huey, gunicorn, graphql-core, graphql-relay, docker, graphene, databricks-sdk, mlflow-tracing, mlflow-skinny, mlflow\n",
            "Successfully installed databricks-sdk-0.76.0 docker-7.1.0 graphene-3.4.3 graphql-core-3.2.7 graphql-relay-3.2.0 gunicorn-23.0.0 huey-2.5.5 mlflow-3.8.1 mlflow-skinny-3.8.1 mlflow-tracing-3.8.1\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import mlflow\n",
        "  print(\"mlflow is already installed\")\n",
        "except:\n",
        "  !pip install mlflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b703a70",
      "metadata": {
        "id": "3b703a70"
      },
      "source": [
        "### Import RapidFire Components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3f8598e1",
      "metadata": {
        "id": "3f8598e1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n",
        "\n",
        "from rapidfireai import Experiment\n",
        "from rapidfireai.evals.automl import List, RFLangChainRagSpec, RFvLLMModelConfig, RFPromptManager, RFGridSearch\n",
        "import re, json\n",
        "from typing import List as listtype, Dict, Any\n",
        "\n",
        "# NB: If you get \"AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'\" from Colab, just rerun this cell"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e16327b",
      "metadata": {
        "id": "9e16327b"
      },
      "source": [
        "### Load Dataset, Rename Columns, and Downsample Data\n",
        "\n",
        "sample_fraction set extremely low (0.1) to avoid Colab disconnect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "ee571098",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "370aa18ce593405aaa581c6cc5efa3f7",
            "d641d0e917664e739094b4c16b750468",
            "2df8533da6304f898cd33bdd4bd67e3b",
            "1cd9a2158d5f4d18ba78d9f3b124e4d8",
            "6ed2d579ac964325892c8d67fba73b1f",
            "5f86d7bb376b42028740717e48b2d05b",
            "3b625de3785e465191291eb815dd82ba",
            "a4637185b0b44744a33e5bfc40ac80b8",
            "bfdd0bf5cd124303ae28010a17e837f6",
            "fcc42f3142b64a8598ef4e6ca8e57ad3",
            "e39f31f4ca4e4cabadf41964e30309dc"
          ]
        },
        "id": "ee571098",
        "outputId": "1777f92c-2c24-47a5-cc8a-16a02045847a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "370aa18ce593405aaa581c6cc5efa3f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 664 queries\n",
            "Found 1721 relevant documents for these queries\n",
            "Sampled 1721 documents from 57638 total\n",
            "Saved to: /content/tutorial_notebooks/rag-contexteng/datasets/fiqa/corpus_sampled.jsonl\n",
            "Filtered qrels to 1721 relevance judgments\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import json, random\n",
        "from pathlib import Path\n",
        "\n",
        "#FiQA dataset\n",
        "dataset_dir = Path(\"/content/tutorial_notebooks/rag-contexteng/datasets\")\n",
        "\n",
        "#Load all the files\n",
        "fiqa_dataset = load_dataset(\"json\", data_files=str(dataset_dir / \"fiqa\" / \"queries.jsonl\"), split=\"train\")\n",
        "fiqa_dataset = fiqa_dataset.rename_columns({\"text\": \"query\", \"_id\": \"query_id\"})\n",
        "qrels = pd.read_csv(str(dataset_dir / \"fiqa\" / \"qrels.tsv\"), sep=\"\\t\")\n",
        "qrels = qrels.rename(\n",
        "    columns={\"query-id\": \"query_id\", \"corpus-id\": \"corpus_id\", \"score\": \"relevance\"}\n",
        ")\n",
        "\n",
        "#Downsample queries and corpus JOINTLY\n",
        "sample_fraction = 0.1\n",
        "rseed = 1\n",
        "random.seed(rseed)\n",
        "\n",
        "# Sample queries\n",
        "sample_size = int(len(fiqa_dataset) * sample_fraction)\n",
        "fiqa_dataset = fiqa_dataset.shuffle(seed=rseed).select(range(sample_size))\n",
        "\n",
        "# Convert query_ids to integers for matching\n",
        "query_ids = set([int(qid) for qid in fiqa_dataset[\"query_id\"]])\n",
        "\n",
        "# All the corpus docs should now be pointing to a relevant query\n",
        "qrels_filtered = qrels[qrels[\"query_id\"].isin(query_ids)]\n",
        "relevant_corpus_ids = set(qrels_filtered[\"corpus_id\"].tolist())\n",
        "\n",
        "print(f\"Using {len(fiqa_dataset)} queries\")\n",
        "print(f\"Found {len(relevant_corpus_ids)} relevant documents for these queries\")\n",
        "\n",
        "# Load corpus and filter to relevant docs\n",
        "input_file = dataset_dir / \"fiqa\" / \"corpus.jsonl\"\n",
        "output_file = dataset_dir / \"fiqa\" / \"corpus_sampled.jsonl\"\n",
        "\n",
        "with open(input_file, 'r') as f:\n",
        "    all_corpus = [json.loads(line) for line in f]\n",
        "\n",
        "# Filter out any irrelevant documents\n",
        "sampled_corpus = [doc for doc in all_corpus if int(doc[\"_id\"]) in relevant_corpus_ids]\n",
        "\n",
        "# Write sampled corpus\n",
        "with open(output_file, 'w') as f:\n",
        "    for doc in sampled_corpus:\n",
        "        f.write(json.dumps(doc) + '\\n')\n",
        "\n",
        "print(f\"Sampled {len(sampled_corpus)} documents from {len(all_corpus)} total\")\n",
        "print(f\"Saved to: {output_file}\")\n",
        "print(f\"Filtered qrels to {len(qrels_filtered)} relevance judgments\")\n",
        "\n",
        "# Update qrels to match\n",
        "qrels = qrels_filtered"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28399289",
      "metadata": {
        "id": "28399289"
      },
      "source": [
        "### Create Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "70816920",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "70816920",
        "outputId": "87f379e4-78d7-4d26-a158-bfd0cec0f4fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created directory for database at /content/rapidfireai/db\n",
            "Experiment exp1-fiqa-rag-colab created with Experiment ID: 1 at /content/rapidfireai/rapidfire_experiments/exp1-fiqa-rag-colab\n",
            "Created directory: /content/rapidfireai/logs/exp1-fiqa-rag-colab\n",
            "üåê Google Colab detected. Ray dashboard URL: https://8855-gpu-t4-s-11pj5o8h5f5f7-b.us-west1-0.prod.colab.dev\n",
            "üåê Google Colab detected. Dispatcher URL: https://8851-gpu-t4-s-11pj5o8h5f5f7-b.us-west1-0.prod.colab.dev\n"
          ]
        }
      ],
      "source": [
        "experiment = Experiment(experiment_name=\"exp1-fiqa-rag-colab\", mode=\"evals\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a73a21ee",
      "metadata": {
        "id": "a73a21ee"
      },
      "source": [
        "### Define Partial Multi-Config Knobs for LangChain part of RAG Pipeline using RapidFire AI Wrapper APIs\n",
        "\n",
        "Note: encoding algorithm here is gpt2 with chunk size 150 and overlap 20 as well as chunk size 200 with overlao 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "02b73586",
      "metadata": {
        "id": "02b73586"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, JSONLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
        "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
        "\n",
        "# Per-Actor batch size for hardware efficiency\n",
        "batch_size = 50\n",
        "\n",
        "\n",
        "rag_gpu = RFLangChainRagSpec(\n",
        "    document_loader=DirectoryLoader(\n",
        "        path=str(dataset_dir / \"fiqa\"),\n",
        "        glob=\"corpus_sampled.jsonl\",\n",
        "        loader_cls=JSONLoader,\n",
        "        loader_kwargs={\n",
        "            \"jq_schema\": \".\",\n",
        "            \"content_key\": \"text\",\n",
        "            \"metadata_func\": lambda record, metadata: {\n",
        "                \"corpus_id\": int(record.get(\"_id\"))\n",
        "            },  # store the document id\n",
        "            \"json_lines\": True,\n",
        "            \"text_content\": False,\n",
        "        },\n",
        "        sample_seed=42,\n",
        "    ),\n",
        "    # chunking strategies with different chunk sizes (data chunking knob varied)\n",
        "    text_splitter=List([\n",
        "            RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "                encoding_name=\"gpt2\", chunk_size=150, chunk_overlap=20\n",
        "            ),\n",
        "            RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "                encoding_name=\"gpt2\", chunk_size=200, chunk_overlap=60\n",
        "            ),\n",
        "        ],\n",
        "    ),\n",
        "    embedding_cls=HuggingFaceEmbeddings,\n",
        "    embedding_kwargs={\n",
        "        \"model_name\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "        \"model_kwargs\": {\"device\": \"cuda:0\"},\n",
        "        \"encode_kwargs\": {\"normalize_embeddings\": True, \"batch_size\": batch_size},\n",
        "    },\n",
        "    vector_store=None,  # uses FAISS by default\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 2},\n",
        "    # 2 reranking strategies with different top-n values (reranking knob varied)\n",
        "    reranker_cls=CrossEncoderReranker,\n",
        "    reranker_kwargs={\n",
        "        \"model_name\": \"cross-encoder/ms-marco-MiniLM-L6-v2\",\n",
        "        \"model_kwargs\": {\"device\": \"cpu\"},\n",
        "        \"top_n\": List([1, 2]),\n",
        "    },\n",
        "    enable_gpu_search=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd6fb0a8",
      "metadata": {
        "id": "dd6fb0a8"
      },
      "source": [
        "### Define Data Processing and Postprocessing Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ecc17276",
      "metadata": {
        "id": "ecc17276"
      },
      "outputs": [],
      "source": [
        "def sample_preprocess_fn(\n",
        "    batch: Dict[str, listtype], rag: RFLangChainRagSpec, prompt_manager: RFPromptManager\n",
        ") -> Dict[str, listtype]:\n",
        "    \"\"\"Function to prepare the final inputs given to the generator model\"\"\"\n",
        "\n",
        "    INSTRUCTIONS = \"Utilize your financial knowledge, give your answer or opinion to the input question or subject matter.\"\n",
        "\n",
        "    # Perform batched retrieval over all queries; returns a list of lists of k documents per query\n",
        "    all_context = rag.get_context(batch_queries=batch[\"query\"], serialize=False)\n",
        "\n",
        "\n",
        "    retrieved_documents = [\n",
        "        [doc.metadata[\"corpus_id\"] for doc in docs] for docs in all_context\n",
        "    ]\n",
        "\n",
        "\n",
        "    serialized_context = rag.serialize_documents(all_context)\n",
        "    batch[\"query_id\"] = [int(query_id) for query_id in batch[\"query_id\"]]\n",
        "    return {\n",
        "        \"prompts\": [\n",
        "            [\n",
        "                {\"role\": \"system\", \"content\": INSTRUCTIONS},\n",
        "                {\n",
        "                    \"role\": \"user\",\n",
        "                    \"content\": f\"Here is some relevant context:\\n{context}. \\nNow answer the following question using the context provided earlier:\\n{question}\",\n",
        "                },\n",
        "            ]\n",
        "            for question, context in zip(batch[\"query\"], serialized_context)\n",
        "        ],\n",
        "        \"retrieved_documents\": retrieved_documents,\n",
        "        **batch,\n",
        "    }\n",
        "\n",
        "\n",
        "def sample_postprocess_fn(batch: Dict[str, listtype]) -> Dict[str, listtype]:\n",
        "    \"\"\"Function to postprocess outputs produced by generator model\"\"\"\n",
        "    batch[\"ground_truth_documents\"] = [\n",
        "        qrels[qrels[\"query_id\"] == query_id][\"corpus_id\"].tolist()\n",
        "        for query_id in batch[\"query_id\"]\n",
        "    ]\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39eb16c3",
      "metadata": {
        "id": "39eb16c3"
      },
      "source": [
        "### Define Custom Eval Metrics Functions\n",
        "Note: MRR is the focus of the experiment which meausres if the #1 ranked value is the correct retrieved document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "22773d53",
      "metadata": {
        "id": "22773d53"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "\n",
        "def compute_ndcg_at_k(retrieved_docs: set, expected_docs: set, k=5):\n",
        "    \"\"\"Utility function to compute NDCG@k\"\"\"\n",
        "    relevance = [1 if doc in expected_docs else 0 for doc in list(retrieved_docs)[:k]]\n",
        "    dcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(relevance))\n",
        "\n",
        "    # IDCG: perfect ranking limited by min(k, len(expected_docs))\n",
        "    ideal_length = min(k, len(expected_docs))\n",
        "    ideal_relevance = [3] * ideal_length + [0] * (k - ideal_length)\n",
        "    idcg = sum(rel / math.log2(i + 2) for i, rel in enumerate(ideal_relevance))\n",
        "\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "\n",
        "def compute_rr(retrieved_docs: set, expected_docs: set):\n",
        "    \"\"\"Utility function to compute Reciprocal Rank (RR) for a single query\"\"\"\n",
        "    rr = 0\n",
        "    for i, retrieved_doc in enumerate(retrieved_docs):\n",
        "        if retrieved_doc in expected_docs:\n",
        "            rr = 1 / (i + 1)\n",
        "            break\n",
        "    return rr\n",
        "\n",
        "\n",
        "def sample_compute_metrics_fn(batch: Dict[str, listtype]) -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Function to compute all eval metrics based on retrievals and/or generations\"\"\"\n",
        "\n",
        "    true_positives, precisions, recalls, f1_scores, ndcgs, rrs = 0, [], [], [], [], []\n",
        "    total_queries = len(batch[\"query\"])\n",
        "\n",
        "    for pred, gt in zip(batch[\"retrieved_documents\"], batch[\"ground_truth_documents\"]):\n",
        "        expected_set = set(gt)\n",
        "        retrieved_set = set(pred)\n",
        "\n",
        "        true_positives = len(expected_set.intersection(retrieved_set))\n",
        "        precision = true_positives / len(retrieved_set) if len(retrieved_set) > 0 else 0\n",
        "        recall = true_positives / len(expected_set) if len(expected_set) > 0 else 0\n",
        "        f1 = (\n",
        "            2 * precision * recall / (precision + recall)\n",
        "            if (precision + recall) > 0\n",
        "            else 0\n",
        "        )\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1)\n",
        "        ndcgs.append(compute_ndcg_at_k(retrieved_set, expected_set, k=5))\n",
        "        rrs.append(compute_rr(retrieved_set, expected_set))\n",
        "## below will return the correct metrics\n",
        "    return {\n",
        "        \"Total\": {\"value\": total_queries},\n",
        "        \"Precision\": {\"value\": sum(precisions) / total_queries},\n",
        "        \"Recall\": {\"value\": sum(recalls) / total_queries},\n",
        "        \"F1 Score\": {\"value\": sum(f1_scores) / total_queries},\n",
        "        \"NDCG@5\": {\"value\": sum(ndcgs) / total_queries},\n",
        "        \"MRR\": {\"value\": sum(rrs) / total_queries},\n",
        "    }\n",
        "\n",
        "\n",
        "def sample_accumulate_metrics_fn(\n",
        "    aggregated_metrics: Dict[str, listtype],\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"Function to accumulate eval metrics across all batches\"\"\"\n",
        "\n",
        "    num_queries_per_batch = [m[\"value\"] for m in aggregated_metrics[\"Total\"]]\n",
        "    total_queries = sum(num_queries_per_batch)\n",
        "    algebraic_metrics = [\"Precision\", \"Recall\", \"F1 Score\", \"NDCG@5\", \"MRR\"]\n",
        "\n",
        "    return {\n",
        "        \"Total\": {\"value\": total_queries},\n",
        "        **{\n",
        "            metric: {\n",
        "                \"value\": sum(\n",
        "                    m[\"value\"] * queries\n",
        "                    for m, queries in zip(\n",
        "                        aggregated_metrics[metric], num_queries_per_batch\n",
        "                    )\n",
        "                )\n",
        "                / total_queries,\n",
        "                \"is_algebraic\": True,\n",
        "                \"value_range\": (0, 1),\n",
        "            }\n",
        "            for metric in algebraic_metrics\n",
        "        },\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c57887bc",
      "metadata": {
        "id": "c57887bc"
      },
      "source": [
        "### Define Partial Multi-Config Knobs for vLLM Generator part of RAG Pipeline using RapidFire AI Wrapper APIs\n",
        "\n",
        " Qwen2.5-0.5B-Instruct (0.5B parameters) is perfect for Colab's memory constraints and feasible for this experiment\n",
        "\n",
        " Here also has the configs below which can be varied like max_model_len, use_fpc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "8f5d0824",
      "metadata": {
        "id": "8f5d0824"
      },
      "outputs": [],
      "source": [
        "vllm_config1 = RFvLLMModelConfig(\n",
        "    model_config={\n",
        "        \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "        \"dtype\": \"half\",\n",
        "        \"gpu_memory_utilization\": 0.25,\n",
        "        \"tensor_parallel_size\": 1,\n",
        "        \"distributed_executor_backend\": \"mp\",\n",
        "        \"enable_chunked_prefill\": False,\n",
        "        \"enable_prefix_caching\": False,\n",
        "        \"max_model_len\": 6000,\n",
        "        \"disable_log_stats\": True,  # Disable vLLM progress logging\n",
        "        \"enforce_eager\": True,\n",
        "        \"disable_custom_all_reduce\": True,\n",
        "    },\n",
        "    sampling_params={\n",
        "        \"temperature\": 0.8,\n",
        "        \"top_p\": 0.95,\n",
        "        \"max_tokens\": 128,\n",
        "    },\n",
        "    rag=rag_gpu,\n",
        "    prompt_manager=None,\n",
        ")\n",
        "\n",
        "batch_size = 3 # Smaller batch size for generation\n",
        "config_set = {\n",
        "    \"vllm_config\": vllm_config1,  # Only 1 generator, but it represents 4 full configs\n",
        "    \"batch_size\": batch_size,\n",
        "    \"preprocess_fn\": sample_preprocess_fn,\n",
        "    \"postprocess_fn\": sample_postprocess_fn,\n",
        "    \"compute_metrics_fn\": sample_compute_metrics_fn,\n",
        "    \"accumulate_metrics_fn\": sample_accumulate_metrics_fn,\n",
        "    \"online_strategy_kwargs\": {\n",
        "        \"strategy_name\": \"normal\",\n",
        "        \"confidence_level\": 0.95,\n",
        "        \"use_fpc\": True,\n",
        "    },\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a7dd280",
      "metadata": {
        "id": "3a7dd280"
      },
      "source": [
        "### Create Config Group"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "67f26d9d",
      "metadata": {
        "id": "67f26d9d"
      },
      "outputs": [],
      "source": [
        "config_group = RFGridSearch(config_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e8a0e92",
      "metadata": {
        "id": "6e8a0e92"
      },
      "source": [
        "### Display Ray Dashboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c7447480",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "c7447480",
        "outputId": "b33adf6a-630d-45ed-fe8d-5a281a9f672c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8855, \"/\", \"100%\", \"400\", false, window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "from google.colab import output\n",
        "output.serve_kernel_port_as_iframe(8855)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa186134",
      "metadata": {
        "id": "fa186134"
      },
      "source": [
        "### Run Multi-Config Evals + Launch Interactive Run Controller\n",
        "\n",
        "\n",
        "RapidFire AI also provides an Interactive Controller panel UI for Colab that lets you manage executing runs dynamically in real-time from the notebook:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "8e07274a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "resources": {
            "http://localhost:8851/dispatcher/list-all-pipeline-ids": {
              "data": "W3sicGlwZWxpbmVfaWQiOjQsInNoYXJkc19jb21wbGV0ZWQiOjAsInN0YXR1cyI6Im5ldyIsInRvdGFsX3NhbXBsZXNfcHJvY2Vzc2VkIjowfSx7InBpcGVsaW5lX2lkIjozLCJzaGFyZHNfY29tcGxldGVkIjowLCJzdGF0dXMiOiJuZXciLCJ0b3RhbF9zYW1wbGVzX3Byb2Nlc3NlZCI6MH0seyJwaXBlbGluZV9pZCI6Miwic2hhcmRzX2NvbXBsZXRlZCI6MCwic3RhdHVzIjoibmV3IiwidG90YWxfc2FtcGxlc19wcm9jZXNzZWQiOjB9LHsicGlwZWxpbmVfaWQiOjEsInNoYXJkc19jb21wbGV0ZWQiOjAsInN0YXR1cyI6Im9uZ29pbmciLCJ0b3RhbF9zYW1wbGVzX3Byb2Nlc3NlZCI6MH1dCg==",
              "headers": [
                [
                  "content-length",
                  "334"
                ],
                [
                  "content-type",
                  "application/json"
                ]
              ],
              "ok": true,
              "status": 200,
              "status_text": ""
            },
            "http://localhost:8851/dispatcher/get-pipeline-config-json/4": {
              "data": "eyJjb250ZXh0X2lkIjoyLCJwaXBlbGluZV9jb25maWdfanNvbiI6eyJiYXRjaF9zaXplIjozLCJtb2RlbF9jb25maWciOnsiZGlzYWJsZV9jdXN0b21fYWxsX3JlZHVjZSI6dHJ1ZSwiZGlzYWJsZV9sb2dfc3RhdHMiOnRydWUsImRpc3RyaWJ1dGVkX2V4ZWN1dG9yX2JhY2tlbmQiOiJtcCIsImR0eXBlIjoiaGFsZiIsImVuYWJsZV9jaHVua2VkX3ByZWZpbGwiOmZhbHNlLCJlbmFibGVfcHJlZml4X2NhY2hpbmciOmZhbHNlLCJlbmZvcmNlX2VhZ2VyIjp0cnVlLCJncHVfbWVtb3J5X3V0aWxpemF0aW9uIjowLjI1LCJtYXhfbW9kZWxfbGVuIjo2MDAwLCJtb2RlbCI6IlF3ZW4vUXdlbjIuNS0wLjVCLUluc3RydWN0IiwidGVuc29yX3BhcmFsbGVsX3NpemUiOjF9LCJvbmxpbmVfc3RyYXRlZ3lfa3dhcmdzIjp7ImNvbmZpZGVuY2VfbGV2ZWwiOjAuOTUsInN0cmF0ZWd5X25hbWUiOiJub3JtYWwiLCJ1c2VfZnBjIjp0cnVlfSwicGlwZWxpbmVfdHlwZSI6InZsbG0iLCJyYWdfY29uZmlnIjp7ImNodW5rX292ZXJsYXAiOjYwLCJjaHVua19zaXplIjoyMDAsImsiOjIsInNlYXJjaF90eXBlIjoic2ltaWxhcml0eSIsInRvcF9uIjoyfSwic2FtcGxpbmdfcGFyYW1zIjp7Im1heF90b2tlbnMiOjEyOCwidGVtcGVyYXR1cmUiOjAuOCwidG9wX3AiOjAuOTV9fX0K",
              "ok": true,
              "headers": [
                [
                  "content-length",
                  "654"
                ],
                [
                  "content-type",
                  "application/json"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          }
        },
        "id": "8e07274a",
        "outputId": "f7f59a43-4e90-4b61-a9ba-f0ede881378e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "        <div id=\"controller_a18836ac\" style=\"font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif; max-width: 900px; margin: 0 auto;\">\n",
              "            <style>\n",
              "                #controller_a18836ac h3 { margin: 10px 0; font-size: 1.2em; font-weight: 600; }\n",
              "                #controller_a18836ac .header-info { display: flex; gap: 20px; margin: 10px 0; padding: 10px; background: #f8f9fa; border-radius: 4px; font-size: 13px; }\n",
              "                #controller_a18836ac .section { margin: 15px 0; }\n",
              "                #controller_a18836ac .section-label { font-weight: 600; margin-bottom: 8px; font-size: 14px; }\n",
              "                #controller_a18836ac .button-row { display: flex; gap: 8px; flex-wrap: wrap; margin: 10px 0; }\n",
              "                #controller_a18836ac select { padding: 6px 12px; border: 1px solid #ccc; border-radius: 4px; font-size: 13px; background: white; min-width: 300px; cursor: pointer; }\n",
              "                #controller_a18836ac button { padding: 6px 16px; border: none; border-radius: 4px; font-size: 13px; font-weight: 500; cursor: pointer; }\n",
              "                #controller_a18836ac button:disabled { opacity: 0.5; cursor: not-allowed; }\n",
              "                #controller_a18836ac .btn-success { background: #28a745; color: white; }\n",
              "                #controller_a18836ac .btn-danger { background: #dc3545; color: white; }\n",
              "                #controller_a18836ac .btn-info { background: #17a2b8; color: white; }\n",
              "                #controller_a18836ac .btn-default { background: #6c757d; color: white; }\n",
              "                #controller_a18836ac textarea { width: 100%; min-height: 200px; padding: 10px; border: 1px solid #ccc; border-radius: 4px; font-family: 'Courier New', monospace; font-size: 12px; box-sizing: border-box; }\n",
              "                #controller_a18836ac .status-message { padding: 10px; margin: 10px 0; border-radius: 4px; display: none; }\n",
              "                #controller_a18836ac .msg-success { background: #d4edda; color: #155724; }\n",
              "                #controller_a18836ac .msg-error { background: #f8d7da; color: #721c24; }\n",
              "                #controller_a18836ac .msg-info { background: #d1ecf1; color: #0c5460; }\n",
              "            </style>\n",
              "\n",
              "            <div>\n",
              "                <h3>Interactive Run Controller</h3>\n",
              "                <div class=\"header-info\">\n",
              "                    <div><b>Run ID:</b> <span id=\"pipeline-id-value\">N/A</span></div>\n",
              "                    <div><b>Status:</b> <span id=\"status-value\">Not loaded</span></div>\n",
              "                    <div><b>Last Update:</b> <span id=\"last-update\">Never</span></div>\n",
              "                </div>\n",
              "\n",
              "                <div id=\"status-message\" class=\"status-message\"></div>\n",
              "\n",
              "                <div class=\"section\">\n",
              "                    <div class=\"section-label\">Select a Config ID:</div>\n",
              "                    <select id=\"pipeline-selector\">\n",
              "                        <option value=\"\">Waiting for data...</option>\n",
              "                    </select>\n",
              "                </div>\n",
              "\n",
              "                <div class=\"section\">\n",
              "                    <div class=\"button-row\">\n",
              "                        <button class=\"btn-success\" id=\"resume-btn\">‚ñ∂ Resume</button>\n",
              "                        <button class=\"btn-danger\" id=\"stop-btn\">‚ñ† Stop</button>\n",
              "                        <button class=\"btn-danger\" id=\"delete-btn\">üóë Delete</button>\n",
              "                    </div>\n",
              "                </div>\n",
              "\n",
              "                <div class=\"section\">\n",
              "                    <div class=\"section-label\">Configuration: <span id=\"config-name\">N/A</span></div>\n",
              "                    <textarea id=\"config-text\" readonly>{}</textarea>\n",
              "                    <div class=\"button-row\">\n",
              "                        <button class=\"btn-info\" id=\"clone-btn\">Clone Run</button>\n",
              "                        <button class=\"btn-success\" id=\"submit-clone-btn\" disabled>‚úì Submit Clone</button>\n",
              "                        <button class=\"btn-danger\" id=\"cancel-clone-btn\" disabled>‚úó Cancel</button>\n",
              "                    </div>\n",
              "                </div>\n",
              "            </div>\n",
              "\n",
              "            <script>\n",
              "                (function() {\n",
              "                    const WIDGET_ID = 'controller_a18836ac';\n",
              "                    const DISPATCHER_URL = 'https://localhost:8851';\n",
              "                    let currentPipelineId = null;\n",
              "                    let currentConfig = null;\n",
              "                    let currentContextId = null;\n",
              "                    let isCloneMode = false;\n",
              "                    let pollingInterval = null;\n",
              "\n",
              "                    // Elements\n",
              "                    const el = {\n",
              "                        pipelineIdValue: document.getElementById('pipeline-id-value'),\n",
              "                        statusValue: document.getElementById('status-value'),\n",
              "                        lastUpdate: document.getElementById('last-update'),\n",
              "                        statusMessage: document.getElementById('status-message'),\n",
              "                        pipelineSelector: document.getElementById('pipeline-selector'),\n",
              "                        resumeBtn: document.getElementById('resume-btn'),\n",
              "                        stopBtn: document.getElementById('stop-btn'),\n",
              "                        deleteBtn: document.getElementById('delete-btn'),\n",
              "                        configName: document.getElementById('config-name'),\n",
              "                        configText: document.getElementById('config-text'),\n",
              "                        cloneBtn: document.getElementById('clone-btn'),\n",
              "                        submitCloneBtn: document.getElementById('submit-clone-btn'),\n",
              "                        cancelCloneBtn: document.getElementById('cancel-clone-btn')\n",
              "                    };\n",
              "\n",
              "                    // Use fetch API with explicit CORS mode and optional auth token\n",
              "                    async function xhrRequest(url, method = 'GET', body = null) {\n",
              "                        const options = {\n",
              "                            method: method,\n",
              "                            headers: {\n",
              "                                'Content-Type': 'application/json'\n",
              "                            },\n",
              "                            mode: 'cors',\n",
              "                            credentials: 'include'  // Include cookies for Colab proxy auth\n",
              "                        };\n",
              "\n",
              "                        if (body) {\n",
              "                            options.body = JSON.stringify(body);\n",
              "                        }\n",
              "\n",
              "                        const response = await fetch(url, options);\n",
              "                        if (!response.ok) {\n",
              "                            throw new Error('HTTP ' + response.status);\n",
              "                        }\n",
              "                        return await response.json();\n",
              "                    }\n",
              "\n",
              "                    async function fetchPipelines() {\n",
              "                        try {\n",
              "                            console.log('Fetching pipelines...');\n",
              "                            const pipelines = await xhrRequest(DISPATCHER_URL + '/dispatcher/list-all-pipeline-ids');\n",
              "                            console.log('Got pipelines:', pipelines.length);\n",
              "\n",
              "                            updatePipelinesDropdown(pipelines);\n",
              "                            el.lastUpdate.textContent = new Date().toLocaleTimeString();\n",
              "\n",
              "                        } catch (error) {\n",
              "                            console.error('Failed to fetch pipelines:', error);\n",
              "                            showMessage('Connection error: ' + error.message, 'error');\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    async function fetchPipelineConfig(pipelineId) {\n",
              "                        try {\n",
              "                            const data = await xhrRequest(DISPATCHER_URL + `/dispatcher/get-pipeline-config-json/${pipelineId}`);\n",
              "                            const config = data.pipeline_config_json || {};\n",
              "\n",
              "                            currentConfig = config;\n",
              "                            currentContextId = data.context_id;\n",
              "\n",
              "                            el.configName.textContent = config.pipeline_name || 'N/A';\n",
              "\n",
              "                            if (!isCloneMode) {\n",
              "                                el.configText.value = JSON.stringify(config, null, 2);\n",
              "                            }\n",
              "\n",
              "                        } catch (error) {\n",
              "                            console.error('Failed to fetch config:', error);\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    function updatePipelinesDropdown(pipelines) {\n",
              "                        const selector = el.pipelineSelector;\n",
              "                        const currentSelection = selector.value;\n",
              "\n",
              "                        selector.innerHTML = '';\n",
              "\n",
              "                        if (pipelines && pipelines.length > 0) {\n",
              "                            pipelines.forEach(p => {\n",
              "                                const option = document.createElement('option');\n",
              "                                option.value = p.pipeline_id;\n",
              "                                option.textContent = `Config ID: ${p.pipeline_id} (${p.status || 'unknown'})`;\n",
              "                                selector.appendChild(option);\n",
              "                            });\n",
              "\n",
              "                            if (currentSelection && pipelines.some(p => p.pipeline_id == currentSelection)) {\n",
              "                                selector.value = currentSelection;\n",
              "                                currentPipelineId = currentSelection;\n",
              "                            } else {\n",
              "                                selector.value = pipelines[0].pipeline_id;\n",
              "                                currentPipelineId = pipelines[0].pipeline_id;\n",
              "                                fetchPipelineConfig(currentPipelineId);\n",
              "                            }\n",
              "\n",
              "                            // Update status display\n",
              "                            const currentPipeline = pipelines.find(p => p.pipeline_id == currentPipelineId);\n",
              "                            if (currentPipeline) {\n",
              "                                el.pipelineIdValue.textContent = currentPipeline.pipeline_id;\n",
              "                                el.statusValue.textContent = currentPipeline.status || 'unknown';\n",
              "\n",
              "                                const isCompleted = currentPipeline.status?.toLowerCase() === 'completed';\n",
              "                                el.resumeBtn.disabled = isCompleted;\n",
              "                                el.stopBtn.disabled = isCompleted;\n",
              "                                el.deleteBtn.disabled = isCompleted;\n",
              "                                el.cloneBtn.disabled = isCompleted || !currentContextId;\n",
              "                            }\n",
              "                        } else {\n",
              "                            selector.innerHTML = '<option value=\"\">No pipelines found</option>';\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    function showMessage(message, type) {\n",
              "                        el.statusMessage.className = 'status-message msg-' + type;\n",
              "                        el.statusMessage.textContent = message;\n",
              "                        el.statusMessage.style.display = 'block';\n",
              "                        setTimeout(() => el.statusMessage.style.display = 'none', 5000);\n",
              "                    }\n",
              "\n",
              "                    async function handleAction(action) {\n",
              "                        if (!currentPipelineId) {\n",
              "                            showMessage('No pipeline selected', 'error');\n",
              "                            return;\n",
              "                        }\n",
              "\n",
              "                        try {\n",
              "                            const endpoint = DISPATCHER_URL + `/dispatcher/${action}-pipeline`;\n",
              "                            const result = await xhrRequest(endpoint, 'POST', { pipeline_id: currentPipelineId });\n",
              "\n",
              "                            showMessage(`‚úì ${action} completed for pipeline ${currentPipelineId}`, 'success');\n",
              "\n",
              "                            // Refresh after a short delay\n",
              "                            setTimeout(async () => {\n",
              "                                await fetchPipelines();\n",
              "                            }, 500);\n",
              "\n",
              "                        } catch (error) {\n",
              "                            showMessage(`Error: ${error.message}`, 'error');\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    function enableCloneMode() {\n",
              "                        isCloneMode = true;\n",
              "                        el.configText.readOnly = false;\n",
              "                        el.submitCloneBtn.disabled = false;\n",
              "                        el.cancelCloneBtn.disabled = false;\n",
              "                        el.cloneBtn.disabled = true;\n",
              "                        showMessage('Edit config and click Submit to clone', 'info');\n",
              "                    }\n",
              "\n",
              "                    function disableCloneMode() {\n",
              "                        isCloneMode = false;\n",
              "                        el.configText.readOnly = true;\n",
              "                        el.configText.value = JSON.stringify(currentConfig || {}, null, 2);\n",
              "                        el.submitCloneBtn.disabled = true;\n",
              "                        el.cancelCloneBtn.disabled = true;\n",
              "                        el.cloneBtn.disabled = false;\n",
              "                    }\n",
              "\n",
              "                    async function handleClone() {\n",
              "                        if (!currentPipelineId) {\n",
              "                            showMessage('No pipeline selected', 'error');\n",
              "                            return;\n",
              "                        }\n",
              "\n",
              "                        try {\n",
              "                            // Parse edited config\n",
              "                            let editedConfig;\n",
              "                            try {\n",
              "                                editedConfig = JSON.parse(el.configText.value);\n",
              "                            } catch (e) {\n",
              "                                showMessage('Invalid JSON: ' + e.message, 'error');\n",
              "                                return;\n",
              "                            }\n",
              "\n",
              "                            // Validate required fields\n",
              "                            if (!editedConfig.pipeline_type) {\n",
              "                                showMessage('config_json must include pipeline_type', 'error');\n",
              "                                return;\n",
              "                            }\n",
              "\n",
              "                            // Send clone request\n",
              "                            const cloneRequest = {\n",
              "                                parent_pipeline_id: currentPipelineId,\n",
              "                                config_json: editedConfig\n",
              "                            };\n",
              "\n",
              "                            const result = await xhrRequest(\n",
              "                                DISPATCHER_URL + '/dispatcher/clone-pipeline',\n",
              "                                'POST',\n",
              "                                cloneRequest\n",
              "                            );\n",
              "\n",
              "                            showMessage(`‚úì Cloned from Config ID ${currentPipelineId} successfully!`, 'success');\n",
              "                            disableCloneMode();\n",
              "\n",
              "                            // Refresh after delay\n",
              "                            setTimeout(async () => {\n",
              "                                await fetchPipelines();\n",
              "                            }, 1000);\n",
              "\n",
              "                        } catch (error) {\n",
              "                            showMessage(`Error cloning: ${error.message}`, 'error');\n",
              "                        }\n",
              "                    }\n",
              "\n",
              "                    // Event listeners\n",
              "                    el.pipelineSelector.addEventListener('change', (e) => {\n",
              "                        if (e.target.value) {\n",
              "                            currentPipelineId = parseInt(e.target.value);\n",
              "                            fetchPipelineConfig(currentPipelineId);\n",
              "                        }\n",
              "                    });\n",
              "\n",
              "                    el.resumeBtn.addEventListener('click', () => handleAction('resume'));\n",
              "                    el.stopBtn.addEventListener('click', () => handleAction('stop'));\n",
              "                    el.deleteBtn.addEventListener('click', () => handleAction('delete'));\n",
              "\n",
              "                    el.cloneBtn.addEventListener('click', enableCloneMode);\n",
              "                    el.submitCloneBtn.addEventListener('click', handleClone);\n",
              "                    el.cancelCloneBtn.addEventListener('click', () => {\n",
              "                        disableCloneMode();\n",
              "                        showMessage('Cancelled clone', 'info');\n",
              "                    });\n",
              "\n",
              "                    // Initial fetch\n",
              "                    console.log('UI initialized, fetching initial data...');\n",
              "                    setTimeout(async () => {\n",
              "                        await fetchPipelines();\n",
              "\n",
              "                        // Start polling - use HTTP fetch (works even when kernel is busy)\n",
              "                        pollingInterval = setInterval(async () => {\n",
              "                            await fetchPipelines();\n",
              "                        }, 3000.0);\n",
              "                        console.log('Polling started: every 3.0s');\n",
              "                    }, 1000);\n",
              "\n",
              "                    // Cleanup on unload\n",
              "                    window.addEventListener('beforeunload', () => {\n",
              "                        if (pollingInterval) clearInterval(pollingInterval);\n",
              "                    });\n",
              "                })();\n",
              "            </script>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Preprocessing RAG Sources ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x783b025b7980>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_9fc55\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_9fc55_level0_col0\" class=\"col_heading level0 col0\" >RAG Source ID</th>\n",
              "      <th id=\"T_9fc55_level0_col1\" class=\"col_heading level0 col1\" >Status</th>\n",
              "      <th id=\"T_9fc55_level0_col2\" class=\"col_heading level0 col2\" >Duration</th>\n",
              "      <th id=\"T_9fc55_level0_col3\" class=\"col_heading level0 col3\" >Details</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_9fc55_row0_col0\" class=\"data row0 col0\" >1</td>\n",
              "      <td id=\"T_9fc55_row0_col1\" class=\"data row0 col1\" >Complete</td>\n",
              "      <td id=\"T_9fc55_row0_col2\" class=\"data row0 col2\" >59.5s</td>\n",
              "      <td id=\"T_9fc55_row0_col3\" class=\"data row0 col3\" >FAISS, GPU</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_9fc55_row1_col0\" class=\"data row1 col0\" >2</td>\n",
              "      <td id=\"T_9fc55_row1_col1\" class=\"data row1 col1\" >Complete</td>\n",
              "      <td id=\"T_9fc55_row1_col2\" class=\"data row1 col2\" >59.9s</td>\n",
              "      <td id=\"T_9fc55_row1_col3\" class=\"data row1 col3\" >FAISS, GPU</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Multi-Config Experiment Progress ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<pandas.io.formats.style.Styler at 0x783b02205eb0>"
            ],
            "text/html": [
              "<style type=\"text/css\">\n",
              "</style>\n",
              "<table id=\"T_58c3c\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th id=\"T_58c3c_level0_col0\" class=\"col_heading level0 col0\" >Run ID</th>\n",
              "      <th id=\"T_58c3c_level0_col1\" class=\"col_heading level0 col1\" >Model</th>\n",
              "      <th id=\"T_58c3c_level0_col2\" class=\"col_heading level0 col2\" >Status</th>\n",
              "      <th id=\"T_58c3c_level0_col3\" class=\"col_heading level0 col3\" >Progress</th>\n",
              "      <th id=\"T_58c3c_level0_col4\" class=\"col_heading level0 col4\" >Conf. Interval</th>\n",
              "      <th id=\"T_58c3c_level0_col5\" class=\"col_heading level0 col5\" >search_type</th>\n",
              "      <th id=\"T_58c3c_level0_col6\" class=\"col_heading level0 col6\" >rag_k</th>\n",
              "      <th id=\"T_58c3c_level0_col7\" class=\"col_heading level0 col7\" >top_n</th>\n",
              "      <th id=\"T_58c3c_level0_col8\" class=\"col_heading level0 col8\" >chunk_size</th>\n",
              "      <th id=\"T_58c3c_level0_col9\" class=\"col_heading level0 col9\" >chunk_overlap</th>\n",
              "      <th id=\"T_58c3c_level0_col10\" class=\"col_heading level0 col10\" >sampling_params</th>\n",
              "      <th id=\"T_58c3c_level0_col11\" class=\"col_heading level0 col11\" >model_config</th>\n",
              "      <th id=\"T_58c3c_level0_col12\" class=\"col_heading level0 col12\" >Precision</th>\n",
              "      <th id=\"T_58c3c_level0_col13\" class=\"col_heading level0 col13\" >Recall</th>\n",
              "      <th id=\"T_58c3c_level0_col14\" class=\"col_heading level0 col14\" >F1 Score</th>\n",
              "      <th id=\"T_58c3c_level0_col15\" class=\"col_heading level0 col15\" >NDCG@5</th>\n",
              "      <th id=\"T_58c3c_level0_col16\" class=\"col_heading level0 col16\" >MRR</th>\n",
              "      <th id=\"T_58c3c_level0_col17\" class=\"col_heading level0 col17\" >Throughput</th>\n",
              "      <th id=\"T_58c3c_level0_col18\" class=\"col_heading level0 col18\" >Total</th>\n",
              "      <th id=\"T_58c3c_level0_col19\" class=\"col_heading level0 col19\" >Samples Processed</th>\n",
              "      <th id=\"T_58c3c_level0_col20\" class=\"col_heading level0 col20\" >Processing Time</th>\n",
              "      <th id=\"T_58c3c_level0_col21\" class=\"col_heading level0 col21\" >Samples Per Second</th>\n",
              "      <th id=\"T_58c3c_level0_col22\" class=\"col_heading level0 col22\" >model_name</th>\n",
              "      <th id=\"T_58c3c_level0_col23\" class=\"col_heading level0 col23\" >run_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td id=\"T_58c3c_row0_col0\" class=\"data row0 col0\" >1</td>\n",
              "      <td id=\"T_58c3c_row0_col1\" class=\"data row0 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_58c3c_row0_col2\" class=\"data row0 col2\" >COMPLETED</td>\n",
              "      <td id=\"T_58c3c_row0_col3\" class=\"data row0 col3\" >4/4</td>\n",
              "      <td id=\"T_58c3c_row0_col4\" class=\"data row0 col4\" >0.000</td>\n",
              "      <td id=\"T_58c3c_row0_col5\" class=\"data row0 col5\" >similarity</td>\n",
              "      <td id=\"T_58c3c_row0_col6\" class=\"data row0 col6\" >2.00</td>\n",
              "      <td id=\"T_58c3c_row0_col7\" class=\"data row0 col7\" >1.00</td>\n",
              "      <td id=\"T_58c3c_row0_col8\" class=\"data row0 col8\" >150.00</td>\n",
              "      <td id=\"T_58c3c_row0_col9\" class=\"data row0 col9\" >20.00</td>\n",
              "      <td id=\"T_58c3c_row0_col10\" class=\"data row0 col10\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 128}</td>\n",
              "      <td id=\"T_58c3c_row0_col11\" class=\"data row0 col11\" >{'dtype': 'half', 'gpu_memory_utilization': 0.25, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'max_model_len': 6000, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True}</td>\n",
              "      <td id=\"T_58c3c_row0_col12\" class=\"data row0 col12\" >55.42% [55.42%, 55.42%]</td>\n",
              "      <td id=\"T_58c3c_row0_col13\" class=\"data row0 col13\" >49.03% [49.03%, 49.03%]</td>\n",
              "      <td id=\"T_58c3c_row0_col14\" class=\"data row0 col14\" >47.65% [47.65%, 47.65%]</td>\n",
              "      <td id=\"T_58c3c_row0_col15\" class=\"data row0 col15\" >16.83% [16.83%, 16.83%]</td>\n",
              "      <td id=\"T_58c3c_row0_col16\" class=\"data row0 col16\" >65.51% [65.51%, 65.51%]</td>\n",
              "      <td id=\"T_58c3c_row0_col17\" class=\"data row0 col17\" >0.2/s</td>\n",
              "      <td id=\"T_58c3c_row0_col18\" class=\"data row0 col18\" >664</td>\n",
              "      <td id=\"T_58c3c_row0_col19\" class=\"data row0 col19\" >664</td>\n",
              "      <td id=\"T_58c3c_row0_col20\" class=\"data row0 col20\" >5164.64 seconds</td>\n",
              "      <td id=\"T_58c3c_row0_col21\" class=\"data row0 col21\" >0.13</td>\n",
              "      <td id=\"T_58c3c_row0_col22\" class=\"data row0 col22\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_58c3c_row0_col23\" class=\"data row0 col23\" >1.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_58c3c_row1_col0\" class=\"data row1 col0\" >2</td>\n",
              "      <td id=\"T_58c3c_row1_col1\" class=\"data row1 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_58c3c_row1_col2\" class=\"data row1 col2\" >COMPLETED</td>\n",
              "      <td id=\"T_58c3c_row1_col3\" class=\"data row1 col3\" >4/4</td>\n",
              "      <td id=\"T_58c3c_row1_col4\" class=\"data row1 col4\" >0.000</td>\n",
              "      <td id=\"T_58c3c_row1_col5\" class=\"data row1 col5\" >similarity</td>\n",
              "      <td id=\"T_58c3c_row1_col6\" class=\"data row1 col6\" >2.00</td>\n",
              "      <td id=\"T_58c3c_row1_col7\" class=\"data row1 col7\" >2.00</td>\n",
              "      <td id=\"T_58c3c_row1_col8\" class=\"data row1 col8\" >150.00</td>\n",
              "      <td id=\"T_58c3c_row1_col9\" class=\"data row1 col9\" >20.00</td>\n",
              "      <td id=\"T_58c3c_row1_col10\" class=\"data row1 col10\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 128}</td>\n",
              "      <td id=\"T_58c3c_row1_col11\" class=\"data row1 col11\" >{'dtype': 'half', 'gpu_memory_utilization': 0.25, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'max_model_len': 6000, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True}</td>\n",
              "      <td id=\"T_58c3c_row1_col12\" class=\"data row1 col12\" >55.42% [55.42%, 55.42%]</td>\n",
              "      <td id=\"T_58c3c_row1_col13\" class=\"data row1 col13\" >49.03% [49.03%, 49.03%]</td>\n",
              "      <td id=\"T_58c3c_row1_col14\" class=\"data row1 col14\" >47.65% [47.65%, 47.65%]</td>\n",
              "      <td id=\"T_58c3c_row1_col15\" class=\"data row1 col15\" >16.83% [16.83%, 16.83%]</td>\n",
              "      <td id=\"T_58c3c_row1_col16\" class=\"data row1 col16\" >65.51% [65.51%, 65.51%]</td>\n",
              "      <td id=\"T_58c3c_row1_col17\" class=\"data row1 col17\" >0.2/s</td>\n",
              "      <td id=\"T_58c3c_row1_col18\" class=\"data row1 col18\" >664</td>\n",
              "      <td id=\"T_58c3c_row1_col19\" class=\"data row1 col19\" >664</td>\n",
              "      <td id=\"T_58c3c_row1_col20\" class=\"data row1 col20\" >4513.67 seconds</td>\n",
              "      <td id=\"T_58c3c_row1_col21\" class=\"data row1 col21\" >0.15</td>\n",
              "      <td id=\"T_58c3c_row1_col22\" class=\"data row1 col22\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_58c3c_row1_col23\" class=\"data row1 col23\" >2.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_58c3c_row2_col0\" class=\"data row2 col0\" >3</td>\n",
              "      <td id=\"T_58c3c_row2_col1\" class=\"data row2 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_58c3c_row2_col2\" class=\"data row2 col2\" >COMPLETED</td>\n",
              "      <td id=\"T_58c3c_row2_col3\" class=\"data row2 col3\" >4/4</td>\n",
              "      <td id=\"T_58c3c_row2_col4\" class=\"data row2 col4\" >0.000</td>\n",
              "      <td id=\"T_58c3c_row2_col5\" class=\"data row2 col5\" >similarity</td>\n",
              "      <td id=\"T_58c3c_row2_col6\" class=\"data row2 col6\" >2.00</td>\n",
              "      <td id=\"T_58c3c_row2_col7\" class=\"data row2 col7\" >1.00</td>\n",
              "      <td id=\"T_58c3c_row2_col8\" class=\"data row2 col8\" >200.00</td>\n",
              "      <td id=\"T_58c3c_row2_col9\" class=\"data row2 col9\" >60.00</td>\n",
              "      <td id=\"T_58c3c_row2_col10\" class=\"data row2 col10\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 128}</td>\n",
              "      <td id=\"T_58c3c_row2_col11\" class=\"data row2 col11\" >{'dtype': 'half', 'gpu_memory_utilization': 0.25, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'max_model_len': 6000, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True}</td>\n",
              "      <td id=\"T_58c3c_row2_col12\" class=\"data row2 col12\" >54.44% [54.44%, 54.44%]</td>\n",
              "      <td id=\"T_58c3c_row2_col13\" class=\"data row2 col13\" >47.88% [47.88%, 47.88%]</td>\n",
              "      <td id=\"T_58c3c_row2_col14\" class=\"data row2 col14\" >46.68% [46.68%, 46.68%]</td>\n",
              "      <td id=\"T_58c3c_row2_col15\" class=\"data row2 col15\" >16.53% [16.53%, 16.53%]</td>\n",
              "      <td id=\"T_58c3c_row2_col16\" class=\"data row2 col16\" >64.53% [64.53%, 64.53%]</td>\n",
              "      <td id=\"T_58c3c_row2_col17\" class=\"data row2 col17\" >0.2/s</td>\n",
              "      <td id=\"T_58c3c_row2_col18\" class=\"data row2 col18\" >664</td>\n",
              "      <td id=\"T_58c3c_row2_col19\" class=\"data row2 col19\" >664</td>\n",
              "      <td id=\"T_58c3c_row2_col20\" class=\"data row2 col20\" >4162.07 seconds</td>\n",
              "      <td id=\"T_58c3c_row2_col21\" class=\"data row2 col21\" >0.16</td>\n",
              "      <td id=\"T_58c3c_row2_col22\" class=\"data row2 col22\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_58c3c_row2_col23\" class=\"data row2 col23\" >3.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td id=\"T_58c3c_row3_col0\" class=\"data row3 col0\" >4</td>\n",
              "      <td id=\"T_58c3c_row3_col1\" class=\"data row3 col1\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_58c3c_row3_col2\" class=\"data row3 col2\" >COMPLETED</td>\n",
              "      <td id=\"T_58c3c_row3_col3\" class=\"data row3 col3\" >4/4</td>\n",
              "      <td id=\"T_58c3c_row3_col4\" class=\"data row3 col4\" >0.000</td>\n",
              "      <td id=\"T_58c3c_row3_col5\" class=\"data row3 col5\" >similarity</td>\n",
              "      <td id=\"T_58c3c_row3_col6\" class=\"data row3 col6\" >2.00</td>\n",
              "      <td id=\"T_58c3c_row3_col7\" class=\"data row3 col7\" >2.00</td>\n",
              "      <td id=\"T_58c3c_row3_col8\" class=\"data row3 col8\" >200.00</td>\n",
              "      <td id=\"T_58c3c_row3_col9\" class=\"data row3 col9\" >60.00</td>\n",
              "      <td id=\"T_58c3c_row3_col10\" class=\"data row3 col10\" >{'temperature': 0.8, 'top_p': 0.95, 'max_tokens': 128}</td>\n",
              "      <td id=\"T_58c3c_row3_col11\" class=\"data row3 col11\" >{'dtype': 'half', 'gpu_memory_utilization': 0.25, 'tensor_parallel_size': 1, 'distributed_executor_backend': 'mp', 'enable_chunked_prefill': False, 'enable_prefix_caching': False, 'max_model_len': 6000, 'disable_log_stats': True, 'enforce_eager': True, 'disable_custom_all_reduce': True}</td>\n",
              "      <td id=\"T_58c3c_row3_col12\" class=\"data row3 col12\" >54.44% [54.44%, 54.44%]</td>\n",
              "      <td id=\"T_58c3c_row3_col13\" class=\"data row3 col13\" >47.88% [47.88%, 47.88%]</td>\n",
              "      <td id=\"T_58c3c_row3_col14\" class=\"data row3 col14\" >46.68% [46.68%, 46.68%]</td>\n",
              "      <td id=\"T_58c3c_row3_col15\" class=\"data row3 col15\" >16.53% [16.53%, 16.53%]</td>\n",
              "      <td id=\"T_58c3c_row3_col16\" class=\"data row3 col16\" >64.53% [64.53%, 64.53%]</td>\n",
              "      <td id=\"T_58c3c_row3_col17\" class=\"data row3 col17\" >0.2/s</td>\n",
              "      <td id=\"T_58c3c_row3_col18\" class=\"data row3 col18\" >664</td>\n",
              "      <td id=\"T_58c3c_row3_col19\" class=\"data row3 col19\" >664</td>\n",
              "      <td id=\"T_58c3c_row3_col20\" class=\"data row3 col20\" >3888.49 seconds</td>\n",
              "      <td id=\"T_58c3c_row3_col21\" class=\"data row3 col21\" >0.17</td>\n",
              "      <td id=\"T_58c3c_row3_col22\" class=\"data row3 col22\" >Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td id=\"T_58c3c_row3_col23\" class=\"data row3 col23\" >4.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Launch evals of all RAG configs in the config_group with swap granularity of 4 chunks\n",
        "results = experiment.run_evals(\n",
        "    config_group=config_group,\n",
        "    dataset=fiqa_dataset,\n",
        "    num_actors=1,\n",
        "    num_shards=4,\n",
        "    seed=42,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here results_df returns the data frame that gives the correct metrics"
      ],
      "metadata": {
        "id": "r285brW5GKvE"
      },
      "id": "r285brW5GKvE"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "127e7b4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "127e7b4a",
        "outputId": "ac46f312-4788-4cf9-9b16-71f353568ef6",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   run_id                  model_name search_type  rag_k  top_n  chunk_size  \\\n",
              "0       1  Qwen/Qwen2.5-0.5B-Instruct  similarity      2      1         150   \n",
              "1       2  Qwen/Qwen2.5-0.5B-Instruct  similarity      2      2         150   \n",
              "2       3  Qwen/Qwen2.5-0.5B-Instruct  similarity      2      1         200   \n",
              "3       4  Qwen/Qwen2.5-0.5B-Instruct  similarity      2      2         200   \n",
              "\n",
              "   chunk_overlap                                    sampling_params  \\\n",
              "0             20  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
              "1             20  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
              "2             60  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
              "3             60  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
              "\n",
              "                                        model_config  Samples Processed  \\\n",
              "0  {'dtype': 'half', 'gpu_memory_utilization': 0....                664   \n",
              "1  {'dtype': 'half', 'gpu_memory_utilization': 0....                664   \n",
              "2  {'dtype': 'half', 'gpu_memory_utilization': 0....                664   \n",
              "3  {'dtype': 'half', 'gpu_memory_utilization': 0....                664   \n",
              "\n",
              "   Processing Time Samples Per Second  Total  Precision    Recall  F1 Score  \\\n",
              "0  5164.64 seconds               0.13    664   0.554217  0.490263  0.476464   \n",
              "1  4513.67 seconds               0.15    664   0.554217  0.490263  0.476464   \n",
              "2  4162.07 seconds               0.16    664   0.544428  0.478755  0.466758   \n",
              "3  3888.49 seconds               0.17    664   0.544428  0.478755  0.466758   \n",
              "\n",
              "     NDCG@5       MRR  \n",
              "0  0.168279  0.655120  \n",
              "1  0.168279  0.655120  \n",
              "2  0.165288  0.645331  \n",
              "3  0.165288  0.645331  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d6b52d0b-1c17-49f6-89e2-286c1dc7cf44\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>run_id</th>\n",
              "      <th>model_name</th>\n",
              "      <th>search_type</th>\n",
              "      <th>rag_k</th>\n",
              "      <th>top_n</th>\n",
              "      <th>chunk_size</th>\n",
              "      <th>chunk_overlap</th>\n",
              "      <th>sampling_params</th>\n",
              "      <th>model_config</th>\n",
              "      <th>Samples Processed</th>\n",
              "      <th>Processing Time</th>\n",
              "      <th>Samples Per Second</th>\n",
              "      <th>Total</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "      <th>NDCG@5</th>\n",
              "      <th>MRR</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>similarity</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>150</td>\n",
              "      <td>20</td>\n",
              "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
              "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
              "      <td>664</td>\n",
              "      <td>5164.64 seconds</td>\n",
              "      <td>0.13</td>\n",
              "      <td>664</td>\n",
              "      <td>0.554217</td>\n",
              "      <td>0.490263</td>\n",
              "      <td>0.476464</td>\n",
              "      <td>0.168279</td>\n",
              "      <td>0.655120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>similarity</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>150</td>\n",
              "      <td>20</td>\n",
              "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
              "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
              "      <td>664</td>\n",
              "      <td>4513.67 seconds</td>\n",
              "      <td>0.15</td>\n",
              "      <td>664</td>\n",
              "      <td>0.554217</td>\n",
              "      <td>0.490263</td>\n",
              "      <td>0.476464</td>\n",
              "      <td>0.168279</td>\n",
              "      <td>0.655120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>similarity</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>200</td>\n",
              "      <td>60</td>\n",
              "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
              "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
              "      <td>664</td>\n",
              "      <td>4162.07 seconds</td>\n",
              "      <td>0.16</td>\n",
              "      <td>664</td>\n",
              "      <td>0.544428</td>\n",
              "      <td>0.478755</td>\n",
              "      <td>0.466758</td>\n",
              "      <td>0.165288</td>\n",
              "      <td>0.645331</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Qwen/Qwen2.5-0.5B-Instruct</td>\n",
              "      <td>similarity</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>200</td>\n",
              "      <td>60</td>\n",
              "      <td>{'temperature': 0.8, 'top_p': 0.95, 'max_token...</td>\n",
              "      <td>{'dtype': 'half', 'gpu_memory_utilization': 0....</td>\n",
              "      <td>664</td>\n",
              "      <td>3888.49 seconds</td>\n",
              "      <td>0.17</td>\n",
              "      <td>664</td>\n",
              "      <td>0.544428</td>\n",
              "      <td>0.478755</td>\n",
              "      <td>0.466758</td>\n",
              "      <td>0.165288</td>\n",
              "      <td>0.645331</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d6b52d0b-1c17-49f6-89e2-286c1dc7cf44')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-d6b52d0b-1c17-49f6-89e2-286c1dc7cf44 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-d6b52d0b-1c17-49f6-89e2-286c1dc7cf44');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-037d5003-b6f6-46f2-85f3-ff1b9bd02b87\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-037d5003-b6f6-46f2-85f3-ff1b9bd02b87')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-037d5003-b6f6-46f2-85f3-ff1b9bd02b87 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_a1ea8d4b-bf66-427f-afce-6cca95d70ab5\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_a1ea8d4b-bf66-427f-afce-6cca95d70ab5 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"run_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2,\n          4,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_name\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Qwen/Qwen2.5-0.5B-Instruct\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"search_type\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"similarity\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rag_k\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2,\n        \"max\": 2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"top_n\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 2,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_size\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 28,\n        \"min\": 150,\n        \"max\": 200,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          200\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"chunk_overlap\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 23,\n        \"min\": 20,\n        \"max\": 60,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          60\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sampling_params\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"model_config\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Samples Processed\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 664,\n        \"max\": 664,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          664\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Processing Time\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"4513.67 seconds\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Samples Per Second\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"0.15\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 664,\n        \"max\": 664,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          664\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Precision\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005651772213452668,\n        \"min\": 0.5444277108433735,\n        \"max\": 0.5542168674698795,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.5444277108433735\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Recall\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.006644078873307125,\n        \"min\": 0.47875547728710377,\n        \"max\": 0.4902633594651667,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.47875547728710377\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"F1 Score\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0056040745123864324,\n        \"min\": 0.46675771517638986,\n        \"max\": 0.47646425696124495,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.46675771517638986\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NDCG@5\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.001726739724328737,\n        \"min\": 0.1652879134389865,\n        \"max\": 0.16827871437297134,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.1652879134389865\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"MRR\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.005651772213452668,\n        \"min\": 0.6453313253012049,\n        \"max\": 0.6551204819277109,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0.6453313253012049\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Convert results dict to DataFrame\n",
        "results_df = pd.DataFrame([\n",
        "    {k: v['value'] if isinstance(v, dict) and 'value' in v else v for k, v in {**metrics_dict, 'run_id': run_id}.items()}\n",
        "    for run_id, (_, metrics_dict) in results.items()\n",
        "])\n",
        "\n",
        "results_df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code checks to see if the results printed out the metrics you like"
      ],
      "metadata": {
        "id": "83pRy7DPGQ8l"
      },
      "id": "83pRy7DPGQ8l"
    },
    {
      "cell_type": "code",
      "source": [
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZbq5jB-5ft8",
        "outputId": "ad40aebf-2ce0-4453-9825-ad6fca505486"
      },
      "id": "vZbq5jB-5ft8",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   run_id                  model_name search_type  rag_k  top_n  chunk_size  \\\n",
            "0       1  Qwen/Qwen2.5-0.5B-Instruct  similarity      2      1         150   \n",
            "1       2  Qwen/Qwen2.5-0.5B-Instruct  similarity      2      2         150   \n",
            "2       3  Qwen/Qwen2.5-0.5B-Instruct  similarity      2      1         200   \n",
            "3       4  Qwen/Qwen2.5-0.5B-Instruct  similarity      2      2         200   \n",
            "\n",
            "   chunk_overlap                                    sampling_params  \\\n",
            "0             20  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
            "1             20  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
            "2             60  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
            "3             60  {'temperature': 0.8, 'top_p': 0.95, 'max_token...   \n",
            "\n",
            "                                        model_config  Samples Processed  \\\n",
            "0  {'dtype': 'half', 'gpu_memory_utilization': 0....                664   \n",
            "1  {'dtype': 'half', 'gpu_memory_utilization': 0....                664   \n",
            "2  {'dtype': 'half', 'gpu_memory_utilization': 0....                664   \n",
            "3  {'dtype': 'half', 'gpu_memory_utilization': 0....                664   \n",
            "\n",
            "   Processing Time Samples Per Second  Total  Precision    Recall  F1 Score  \\\n",
            "0  5164.64 seconds               0.13    664   0.554217  0.490263  0.476464   \n",
            "1  4513.67 seconds               0.15    664   0.554217  0.490263  0.476464   \n",
            "2  4162.07 seconds               0.16    664   0.544428  0.478755  0.466758   \n",
            "3  3888.49 seconds               0.17    664   0.544428  0.478755  0.466758   \n",
            "\n",
            "     NDCG@5       MRR  \n",
            "0  0.168279  0.655120  \n",
            "1  0.168279  0.655120  \n",
            "2  0.165288  0.645331  \n",
            "3  0.165288  0.645331  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This adds plots to the folder in colab that can be viewed"
      ],
      "metadata": {
        "id": "to6OWeD8GePP"
      },
      "id": "to6OWeD8GePP"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "w9SViGisMxPJ",
      "metadata": {
        "id": "w9SViGisMxPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9abffa6a-c3b6-4b07-8a90-92ebaf75d705"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/12/30 23:18:50 INFO mlflow.store.db.utils: Creating initial MLflow database tables...\n",
            "2025/12/30 23:18:50 INFO mlflow.store.db.utils: Updating database tables\n",
            "2025/12/30 23:18:50 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2025/12/30 23:18:50 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n",
            "2025/12/30 23:18:50 INFO alembic.runtime.migration: Running upgrade  -> 451aebb31d03, add metric step\n",
            "2025/12/30 23:18:50 INFO alembic.runtime.migration: Running upgrade 451aebb31d03 -> 90e64c465722, migrate user column to tags\n",
            "2025/12/30 23:18:50 INFO alembic.runtime.migration: Running upgrade 90e64c465722 -> 181f10493468, allow nulls for metric values\n",
            "2025/12/30 23:18:50 INFO alembic.runtime.migration: Running upgrade 181f10493468 -> df50e92ffc5e, Add Experiment Tags Table\n",
            "2025/12/30 23:18:50 INFO alembic.runtime.migration: Running upgrade df50e92ffc5e -> 7ac759974ad8, Update run tags with larger limit\n",
            "2025/12/30 23:18:50 INFO alembic.runtime.migration: Running upgrade 7ac759974ad8 -> 89d4b8295536, create latest metrics table\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 89d4b8295536 -> 2b4d017a5e9b, add model registry tables to db\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 2b4d017a5e9b -> cfd24bdc0731, Update run status constraint with killed\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade cfd24bdc0731 -> 0a8213491aaa, drop_duplicate_killed_constraint\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 0a8213491aaa -> 728d730b5ebd, add registered model tags table\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 728d730b5ebd -> 27a6a02d2cf1, add model version tags table\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 27a6a02d2cf1 -> 84291f40a231, add run_link to model_version\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 84291f40a231 -> a8c4a736bde6, allow nulls for run_id\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade a8c4a736bde6 -> 39d1c3be5f05, add_is_nan_constraint_for_metrics_tables_if_necessary\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 39d1c3be5f05 -> c48cb773bb87, reset_default_value_for_is_nan_in_metrics_table_for_mysql\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade c48cb773bb87 -> bd07f7e963c5, create index on run_uuid\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade bd07f7e963c5 -> 0c779009ac13, add deleted_time field to runs table\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 0c779009ac13 -> cc1f77228345, change param value length to 500\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade cc1f77228345 -> 97727af70f4d, Add creation_time and last_update_time to experiments table\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 97727af70f4d -> 3500859a5d39, Add Model Aliases table\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 3500859a5d39 -> 7f2a7d5fae7d, add datasets inputs input_tags tables\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 7f2a7d5fae7d -> 2d6e25af4d3e, increase max param val length from 500 to 8000\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 2d6e25af4d3e -> acf3f17fdcc7, add storage location field to model versions\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade acf3f17fdcc7 -> 867495a8f9d4, add trace tables\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 867495a8f9d4 -> 5b0e9adcef9c, add cascade deletion to trace tables foreign keys\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 5b0e9adcef9c -> 4465047574b1, increase max dataset schema size\n",
            "2025/12/30 23:18:51 INFO alembic.runtime.migration: Running upgrade 4465047574b1 -> f5a4f2784254, increase run tag value limit to 8000\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade f5a4f2784254 -> 0584bdc529eb, add cascading deletion to datasets from experiments\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade 0584bdc529eb -> 400f98739977, add logged model tables\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade 400f98739977 -> 6953534de441, add step to inputs table\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade 6953534de441 -> bda7b8c39065, increase_model_version_tag_value_limit\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade bda7b8c39065 -> cbc13b556ace, add V3 trace schema columns\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade cbc13b556ace -> 770bee3ae1dd, add assessments table\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade 770bee3ae1dd -> a1b2c3d4e5f6, add spans table\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade a1b2c3d4e5f6 -> de4033877273, create entity_associations table\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade de4033877273 -> 1a0cddfcaa16, Add webhooks and webhook_events tables\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade 1a0cddfcaa16 -> 534353b11cbc, add scorer tables\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade 534353b11cbc -> 71994744cf8e, add evaluation datasets\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade 71994744cf8e -> 3da73c924c2f, add outputs to dataset record\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade 3da73c924c2f -> bf29a5ff90ea, add jobs table\n",
            "2025/12/30 23:18:52 INFO alembic.runtime.migration: Running upgrade bf29a5ff90ea -> 1bd49d398cd23, add secrets tables\n",
            "2025/12/30 23:18:53 INFO alembic.runtime.migration: Context impl SQLiteImpl.\n",
            "2025/12/30 23:18:53 INFO alembic.runtime.migration: Will assume non-transactional DDL.\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import mlflow\n",
        "\n",
        "with mlflow.start_run():\n",
        "    for metric in [\"Precision\", \"Recall\", \"F1 Score\", \"NDCG@5\", \"MRR\"]:\n",
        "        plt.figure()\n",
        "        plt.plot(results_df[\"run_id\"], results_df[metric], marker=\"o\")\n",
        "        plt.title(f\"{metric} vs run\")\n",
        "        plt.xlabel(\"Run ID\")\n",
        "        plt.ylabel(metric.upper())\n",
        "        plt.grid(True)\n",
        "        fname = f\"{metric}_plot.png\"\n",
        "        plt.savefig(fname)\n",
        "        mlflow.log_artifact(fname)\n",
        "        plt.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9135d951",
      "metadata": {
        "id": "9135d951"
      },
      "source": [
        "### End Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "94ab038d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 94
        },
        "id": "94ab038d",
        "outputId": "af8ec5c9-b6f2-4362-f186-6cf2d5bd7ec6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<button id=\"continue-btn\" style=\"padding: 10px 20px; font-size: 16px;\">Click to End Experiment</button>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment exp1-fiqa-rag-colab ended\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "from google.colab import output\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "display(HTML('''\n",
        "<button id=\"continue-btn\" style=\"padding: 10px 20px; font-size: 16px;\">Click to End Experiment</button>\n",
        "'''))\n",
        "\n",
        "# eval_js blocks until the Promise resolves\n",
        "output.eval_js('''\n",
        "new Promise((resolve) => {\n",
        "    document.getElementById(\"continue-btn\").onclick = () => {\n",
        "        document.getElementById(\"continue-btn\").disabled = true;\n",
        "        document.getElementById(\"continue-btn\").innerText = \"Continuing...\";\n",
        "        resolve(\"clicked\");\n",
        "    };\n",
        "})\n",
        "''')\n",
        "\n",
        "# Actually end the experiment after the button is clicked\n",
        "experiment.end()\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09265e66",
      "metadata": {
        "id": "09265e66"
      },
      "source": [
        "### View RapidFire AI Log Files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "05379a93",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05379a93",
        "outputId": "f8c7942a-a2e4-4db5-eecc-34636c021192"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Log File: /content/rapidfireai/logs/exp1-fiqa-rag-colab/rapidfire.log\n",
            "\n",
            "================================================================================\n",
            "Last 30 lines of rapidfire.log:\n",
            "================================================================================\n",
            "2025-12-30 23:08:30 | QueryProcessingActor-0 | INFO | query_actor.py:169 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Deserializing FAISS index for this actor...\n",
            "2025-12-30 23:08:30 | sentence_transformers.SentenceTransformer | INFO | SentenceTransformer.py:227 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
            "2025-12-30 23:08:32 | QueryProcessingActor-0 | INFO | query_actor.py:178 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Recreated embedding function: HuggingFaceEmbeddings\n",
            "2025-12-30 23:08:32 | QueryProcessingActor-0 | INFO | query_actor.py:187 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Created independent FAISS vector store for this actor\n",
            "2025-12-30 23:08:32 | QueryProcessingActor-0 | INFO | query_actor.py:196 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Recreated retriever with search_type=similarity\n",
            "2025-12-30 23:08:32 | QueryProcessingActor-0 | INFO | query_actor.py:218 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Recreated RAG spec with retriever and template\n",
            "2025-12-30 23:13:14 | Controller | INFO | controller.py:951 | [exp1-fiqa-rag-colab:Controller] Pipeline 3 completed all 4 shards\n",
            "2025-12-30 23:13:14 | Controller | INFO | controller.py:1003 | [exp1-fiqa-rag-colab:Controller] Pipeline 3 completed shard 3 (56 batches, 281.84s)\n",
            "2025-12-30 23:13:14 | Controller | INFO | controller.py:1105 | [exp1-fiqa-rag-colab:Controller] Scheduling pipeline 4 (Pipeline 4) on actor 0 for shard 3 (56 batches)\n",
            "2025-12-30 23:13:14 | QueryProcessingActor-0 | INFO | query_actor.py:143 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Reusing existing inference engine (config hash: 2acc9e30)\n",
            "2025-12-30 23:13:14 | QueryProcessingActor-0 | INFO | query_actor.py:162 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Using CPU-based FAISS for retrieval (avoids GPU memory conflicts)\n",
            "2025-12-30 23:13:14 | QueryProcessingActor-0 | INFO | query_actor.py:169 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Deserializing FAISS index for this actor...\n",
            "2025-12-30 23:13:14 | sentence_transformers.SentenceTransformer | INFO | SentenceTransformer.py:227 | Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
            "2025-12-30 23:13:15 | QueryProcessingActor-0 | INFO | query_actor.py:178 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Recreated embedding function: HuggingFaceEmbeddings\n",
            "2025-12-30 23:13:15 | QueryProcessingActor-0 | INFO | query_actor.py:187 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Created independent FAISS vector store for this actor\n",
            "2025-12-30 23:13:15 | QueryProcessingActor-0 | INFO | query_actor.py:196 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Recreated retriever with search_type=similarity\n",
            "2025-12-30 23:13:15 | QueryProcessingActor-0 | INFO | query_actor.py:218 | [exp1-fiqa-rag-colab:QueryProcessingActor-0] Recreated RAG spec with retriever and template\n",
            "2025-12-30 23:18:42 | Controller | INFO | controller.py:951 | [exp1-fiqa-rag-colab:Controller] Pipeline 4 completed all 4 shards\n",
            "2025-12-30 23:18:43 | Controller | INFO | controller.py:1003 | [exp1-fiqa-rag-colab:Controller] Pipeline 4 completed shard 3 (56 batches, 326.90s)\n",
            "2025-12-30 23:18:43 | Controller | INFO | controller.py:1069 | [exp1-fiqa-rag-colab:Controller] All pipelines completed all shards!\n",
            "2025-12-30 23:18:43 | Controller | INFO | controller.py:534 | [exp1-fiqa-rag-colab:Controller] Computing final metrics for all pipelines...\n",
            "2025-12-30 23:18:43 | Controller | INFO | controller.py:639 | [exp1-fiqa-rag-colab:Controller] Pipeline 1 (Pipeline 1) completed successfully\n",
            "2025-12-30 23:18:43 | Controller | INFO | controller.py:639 | [exp1-fiqa-rag-colab:Controller] Pipeline 2 (Pipeline 2) completed successfully\n",
            "2025-12-30 23:18:43 | Controller | INFO | controller.py:639 | [exp1-fiqa-rag-colab:Controller] Pipeline 3 (Pipeline 3) completed successfully\n",
            "2025-12-30 23:18:43 | Controller | INFO | controller.py:639 | [exp1-fiqa-rag-colab:Controller] Pipeline 4 (Pipeline 4) completed successfully\n",
            "2025-12-30 23:19:44 | ExperimentUtils | INFO | experiment_utils.py:179 | [exp1-fiqa-rag-colab:ExperimentUtils] Reset experiment states - marked ongoing pipelines, contexts, and tasks as failed\n",
            "2025-12-30 23:19:44 | ExperimentUtils | INFO | experiment_utils.py:187 | [exp1-fiqa-rag-colab:ExperimentUtils] Experiment marked as cancelled. Ongoing pipelines, contexts, and tasks have been marked as failed.\n",
            "2025-12-30 23:19:44 | ExperimentUtils | INFO | experiment_utils.py:153 | [exp1-fiqa-rag-colab:ExperimentUtils] Experiment exp1-fiqa-rag-colab ended\n",
            "2025-12-30 23:19:45 | Experiment | INFO | experiment.py:536 | [exp1-fiqa-rag-colab:Experiment] All actors shut down\n",
            "2025-12-30 23:19:45 | Experiment | INFO | experiment.py:537 | [exp1-fiqa-rag-colab:Experiment] Dispatcher will automatically shut down (daemon thread)\n"
          ]
        }
      ],
      "source": [
        "# Get the experiment-specific log file\n",
        "log_file = experiment.get_log_file_path()\n",
        "\n",
        "print(f\"üìÑ Log File: {log_file}\")\n",
        "print()\n",
        "\n",
        "if log_file.exists():\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"Last 30 lines of {log_file.name}:\")\n",
        "    print(\"=\" * 80)\n",
        "    with open(log_file, 'r', encoding='utf-8') as f:\n",
        "        lines = f.readlines()\n",
        "        for line in lines[-30:]:\n",
        "            print(line.rstrip())\n",
        "else:\n",
        "    print(f\"‚ùå Log file not found: {log_file}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "370aa18ce593405aaa581c6cc5efa3f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d641d0e917664e739094b4c16b750468",
              "IPY_MODEL_2df8533da6304f898cd33bdd4bd67e3b",
              "IPY_MODEL_1cd9a2158d5f4d18ba78d9f3b124e4d8"
            ],
            "layout": "IPY_MODEL_6ed2d579ac964325892c8d67fba73b1f"
          }
        },
        "d641d0e917664e739094b4c16b750468": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f86d7bb376b42028740717e48b2d05b",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_3b625de3785e465191291eb815dd82ba",
            "value": "Generating‚Äátrain‚Äásplit:‚Äá"
          }
        },
        "2df8533da6304f898cd33bdd4bd67e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4637185b0b44744a33e5bfc40ac80b8",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bfdd0bf5cd124303ae28010a17e837f6",
            "value": 1
          }
        },
        "1cd9a2158d5f4d18ba78d9f3b124e4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcc42f3142b64a8598ef4e6ca8e57ad3",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_e39f31f4ca4e4cabadf41964e30309dc",
            "value": "‚Äá6648/0‚Äá[00:00&lt;00:00,‚Äá121553.89‚Äáexamples/s]"
          }
        },
        "6ed2d579ac964325892c8d67fba73b1f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f86d7bb376b42028740717e48b2d05b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3b625de3785e465191291eb815dd82ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a4637185b0b44744a33e5bfc40ac80b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "bfdd0bf5cd124303ae28010a17e837f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcc42f3142b64a8598ef4e6ca8e57ad3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e39f31f4ca4e4cabadf41964e30309dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}